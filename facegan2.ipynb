{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_AtLlx2AJ-N",
        "outputId": "184a4731-8243-4827-fea8-e081ceb5151d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Installing Enhanced Dependencies ===\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✓ Core packages installed\n",
            "Cloning stylegan2-ada-pytorch...\n",
            "Cloning encoder4editing...\n",
            "Cloning SAM...\n",
            "Cloning eg3d...\n",
            "Cloning interfacegan...\n",
            "✓ All repositories cloned\n",
            "\n",
            "=== Downloading Pre-trained Models ===\n",
            "Downloading StyleGAN2 FFHQ...\n",
            "✓ StyleGAN2 FFHQ downloaded\n",
            "Downloading e4e Encoder...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO\n",
            "From (redirected): https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO&confirm=t&uuid=5461c4ab-dbc7-4f1c-b8c8-fcf144a47515\n",
            "To: /content/models/e4e_ffhq_encode.pt\n",
            "100%|██████████| 1.20G/1.20G [00:27<00:00, 43.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ e4e Encoder downloaded\n",
            "Downloading SAM Age Model...\n",
            "⚠ Error downloading SAM Age Model: Failed to retrieve file url:\n",
            "\n",
            "\tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
            "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\thttps://drive.google.com/uc?id=1XyumF6_MBlCAkEmPiXp6hRfA-ZWLdeBs\n",
            "\n",
            "but Gdown can't. Please check connections and permissions.\n",
            "Downloading EG3D FFHQ Model...\n",
            "✓ EG3D FFHQ Model downloaded\n",
            "⚠ Note: You may need to compute InterfaceGAN boundaries for your specific use case\n",
            "\n",
            "✓ Model download complete\n",
            "\n",
            "✓ Using device: cuda\n",
            "  GPU: Tesla T4\n",
            "  Memory: 15.83 GB\n",
            "\n",
            "=== Loading Models with Memory Management ===\n"
          ]
        }
      ],
      "source": [
        "print(\"=== Installing Enhanced Dependencies ===\")\n",
        "\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install ninja --quiet\n",
        "!pip install ftfy regex tqdm --quiet\n",
        "!pip install git+https://github.com/openai/CLIP.git --quiet\n",
        "!pip install lpips --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "!pip install face-alignment --quiet\n",
        "\n",
        "print(\"✓ Core packages installed\")\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "\n",
        "repos = {\n",
        "    'stylegan2-ada-pytorch': 'https://github.com/NVlabs/stylegan2-ada-pytorch.git',\n",
        "    'encoder4editing': 'https://github.com/omertov/encoder4editing.git',\n",
        "    'SAM': 'https://github.com/yuval-alaluf/SAM.git',\n",
        "    'eg3d': 'https://github.com/NVlabs/eg3d.git',\n",
        "    'interfacegan': 'https://github.com/genforce/interfacegan.git'\n",
        "}\n",
        "\n",
        "for name, url in repos.items():\n",
        "    if not os.path.exists(f'/content/{name}'):\n",
        "        print(f\"Cloning {name}...\")\n",
        "        !git clone {url} /content/{name} --quiet\n",
        "\n",
        "print(\"✓ All repositories cloned\")\n",
        "\n",
        "print(\"\\n=== Downloading Pre-trained Models ===\")\n",
        "\n",
        "import gdown\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "MODEL_DIR = Path('/content/models')\n",
        "MODEL_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def download_file(url, output_path, description):\n",
        "    if not Path(output_path).exists():\n",
        "        print(f\"Downloading {description}...\")\n",
        "        try:\n",
        "            if 'drive.google.com' in url:\n",
        "                gdown.download(url, str(output_path), quiet=False, fuzzy=True)\n",
        "            else:\n",
        "                urllib.request.urlretrieve(url, output_path)\n",
        "            print(f\"✓ {description} downloaded\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠ Error downloading {description}: {e}\")\n",
        "    else:\n",
        "        print(f\"✓ {description} already exists\")\n",
        "\n",
        "stylegan_path = MODEL_DIR / 'stylegan2-ffhq-config-f.pkl'\n",
        "download_file(\n",
        "    'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl',\n",
        "    stylegan_path,\n",
        "    'StyleGAN2 FFHQ'\n",
        ")\n",
        "\n",
        "e4e_path = MODEL_DIR / 'e4e_ffhq_encode.pt'\n",
        "download_file(\n",
        "    'https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO',\n",
        "    e4e_path,\n",
        "    'e4e Encoder'\n",
        ")\n",
        "\n",
        "sam_path = MODEL_DIR / 'sam_ffhq_aging.pt'\n",
        "download_file(\n",
        "    'https://drive.google.com/uc?id=1XyumF6_MBlCAkEmPiXp6hRfA-ZWLdeBs',\n",
        "    sam_path,\n",
        "    'SAM Age Model'\n",
        ")\n",
        "\n",
        "eg3d_path = MODEL_DIR / 'ffhq512-128.pkl'\n",
        "download_file(\n",
        "    'https://api.ngc.nvidia.com/v2/models/nvidia/research/eg3d/versions/1/files/ffhq512-128.pkl',\n",
        "    eg3d_path,\n",
        "    'EG3D FFHQ Model'\n",
        ")\n",
        "\n",
        "boundaries_dir = MODEL_DIR / 'interfacegan_boundaries'\n",
        "boundaries_dir.mkdir(exist_ok=True)\n",
        "\n",
        "boundaries = {\n",
        "    'age': 'https://drive.google.com/uc?id=1FJRwzAkV-XWbxFeKIGtZn_S9Cs2VCpz7',\n",
        "    'smile': 'https://drive.google.com/uc?id=1S8gXBfp0f0JGJpfWqJlQnNJ5Xvvwsn6N',\n",
        "    'pose': 'https://drive.google.com/uc?id=1o_Y2dKlPyP8Xzrq6d9oM3uIRIzfLLrC7',\n",
        "}\n",
        "\n",
        "print(\"⚠ Note: You may need to compute InterfaceGAN boundaries for your specific use case\")\n",
        "\n",
        "print(\"\\n✓ Model download complete\")\n",
        "\n",
        "import sys\n",
        "sys.path.extend([\n",
        "    '/content/stylegan2-ada-pytorch',\n",
        "    '/content/encoder4editing',\n",
        "    '/content/SAM',\n",
        "    '/content/eg3d',\n",
        "    '/content/interfacegan'\n",
        "])\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import pickle\n",
        "import clip\n",
        "import lpips\n",
        "from typing import List, Tuple, Optional\n",
        "import dnnlib\n",
        "import legacy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\n✓ Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "print(\"\\n=== Loading Models with Memory Management ===\")\n",
        "\n",
        "class ModelManager:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.device = device\n",
        "\n",
        "    def load_stylegan2(self):\n",
        "        if 'stylegan2' not in self.models:\n",
        "            print(\"Loading StyleGAN2...\")\n",
        "            with open(MODEL_DIR / 'stylegan2-ffhq-config-f.pkl', 'rb') as f:\n",
        "                self.models['stylegan2'] = pickle.load(f)['G_ema'].to(self.device).eval()\n",
        "            print(\"✓ StyleGAN2 loaded\")\n",
        "        return self.models['stylegan2']\n",
        "\n",
        "    def load_e4e(self):\n",
        "        if 'e4e' not in self.models:\n",
        "            print(\"Loading e4e encoder...\")\n",
        "            sys.path.append('/content/encoder4editing')\n",
        "            from models.psp import pSp\n",
        "\n",
        "            ckpt = torch.load(MODEL_DIR / 'e4e_ffhq_encode.pt', map_location='cpu')\n",
        "            opts = ckpt['opts']\n",
        "            opts['checkpoint_path'] = str(MODEL_DIR / 'e4e_ffhq_encode.pt')\n",
        "            opts['device'] = str(self.device)\n",
        "\n",
        "            self.models['e4e'] = pSp(opts).to(self.device).eval()\n",
        "            print(\"✓ e4e encoder loaded\")\n",
        "        return self.models['e4e']\n",
        "\n",
        "    def load_sam(self):\n",
        "        if 'sam' not in self.models:\n",
        "            print(\"Loading SAM model...\")\n",
        "            try:\n",
        "                sys.path.append('/content/SAM')\n",
        "                from models.psp import pSp as SAM_pSp\n",
        "\n",
        "                ckpt = torch.load(MODEL_DIR / 'sam_ffhq_aging.pt', map_location='cpu')\n",
        "                opts = ckpt['opts']\n",
        "                opts['checkpoint_path'] = str(MODEL_DIR / 'sam_ffhq_aging.pt')\n",
        "                opts['device'] = str(self.device)\n",
        "\n",
        "                self.models['sam'] = SAM_pSp(opts).to(self.device).eval()\n",
        "                print(\"✓ SAM model loaded\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ SAM model loading failed: {e}\")\n",
        "                print(\"  Falling back to direction-based age manipulation\")\n",
        "                self.models['sam'] = None\n",
        "        return self.models['sam']\n",
        "\n",
        "    def load_eg3d(self):\n",
        "        if 'eg3d' not in self.models:\n",
        "            print(\"Loading EG3D...\")\n",
        "            try:\n",
        "                with dnnlib.util.open_url(str(MODEL_DIR / 'ffhq512-128.pkl')) as f:\n",
        "                    self.models['eg3d'] = legacy.load_network_pkl(f)['G_ema'].to(self.device).eval()\n",
        "                print(\"✓ EG3D loaded\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠ EG3D loading failed: {e}\")\n",
        "                print(\"  Using StyleGAN2 with limited pose control\")\n",
        "                self.models['eg3d'] = None\n",
        "        return self.models['eg3d']\n",
        "\n",
        "    def load_clip(self):\n",
        "        if 'clip' not in self.models:\n",
        "            print(\"Loading CLIP...\")\n",
        "            model, preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
        "            self.models['clip'] = model\n",
        "            self.models['clip_preprocess'] = preprocess\n",
        "            print(\"✓ CLIP loaded\")\n",
        "        return self.models['clip'], self.models['clip_preprocess']\n",
        "\n",
        "    def load_lpips(self):\n",
        "        if 'lpips' not in self.models:\n",
        "            print(\"Loading LPIPS...\")\n",
        "            self.models['lpips'] = lpips.LPIPS(net='vgg').to(self.device)\n",
        "            print(\"✓ LPIPS loaded\")\n",
        "        return self.models['lpips']\n",
        "\n",
        "    def unload_model(self, model_name):\n",
        "        if model_name in self.models:\n",
        "            del self.models[model_name]\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ {model_name} unloaded\")\n",
        "\n",
        "    def clear_all(self):\n",
        "        self.models.clear()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"✓ All models cleared\")\n",
        "\n",
        "model_manager = ModelManager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS7PXsuYGLRf",
        "outputId": "fe3cea70-47dc-4c0a-bb95-45ac93c724c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Setting up e4e Encoder ===\n",
            "✓ e4e inverter ready\n",
            "\n",
            "=== Setting up InterfaceGAN ===\n",
            "✓ InterfaceGAN setup complete\n",
            "\n",
            "=== Setting up SAM Age Manipulator ===\n",
            "✓ SAM age manipulator ready\n",
            "\n",
            "=== Setting up EG3D Pose Manipulator ===\n",
            "✓ EG3D pose manipulator ready\n",
            "\n",
            "=== Setting up Identity Preservation ===\n",
            "✓ Identity preservation ready\n"
          ]
        }
      ],
      "source": [
        "def load_image(image_path, size=1024):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((size, size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    return transform(img).unsqueeze(0)\n",
        "\n",
        "def tensor_to_pil(tensor):\n",
        "    tensor = (tensor.clamp(-1, 1) + 1) / 2.0\n",
        "    if tensor.dim() == 4:\n",
        "        tensor = tensor.squeeze(0)\n",
        "    tensor = tensor.permute(1, 2, 0).cpu().numpy()\n",
        "    return Image.fromarray((tensor * 255).astype(np.uint8))\n",
        "\n",
        "def display_images(images, titles=None, figsize=(20, 5)):\n",
        "    n = len(images)\n",
        "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
        "    if n == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (img, ax) in enumerate(zip(images, axes)):\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = tensor_to_pil(img)\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "        if titles:\n",
        "            ax.set_title(titles[i], fontsize=10)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_image(tensor, path):\n",
        "    img = tensor_to_pil(tensor)\n",
        "    img.save(path)\n",
        "    print(f\"✓ Saved: {path}\")\n",
        "\n",
        "print(\"\\n=== Setting up e4e Encoder ===\")\n",
        "\n",
        "class E4EInverter:\n",
        "    def __init__(self, model_manager):\n",
        "        self.manager = model_manager\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "    def invert(self, image_path):\n",
        "        print(f\"Inverting image: {image_path}\")\n",
        "\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img_tensor = self.transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        e4e = self.manager.load_e4e()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            latent = e4e(img_tensor, resize=False, return_latents=True)\n",
        "\n",
        "        print(\"✓ Image inverted successfully\")\n",
        "        return latent, img_tensor\n",
        "\n",
        "    def invert_batch(self, image_paths: List[str]):\n",
        "        latents = []\n",
        "        for path in image_paths:\n",
        "            latent, _ = self.invert(path)\n",
        "            latents.append(latent)\n",
        "        return torch.cat(latents, dim=0)\n",
        "\n",
        "e4e_inverter = E4EInverter(model_manager)\n",
        "print(\"✓ e4e inverter ready\")\n",
        "\n",
        "print(\"\\n=== Setting up InterfaceGAN ===\")\n",
        "\n",
        "class InterfaceGANManipulator:\n",
        "    def __init__(self, model_manager, boundaries_dir):\n",
        "        self.manager = model_manager\n",
        "        self.boundaries_dir = Path(boundaries_dir)\n",
        "        self.boundaries = {}\n",
        "\n",
        "    def compute_boundary(self, attribute_name, positive_latents, negative_latents):\n",
        "        from sklearn import svm\n",
        "\n",
        "        print(f\"Computing boundary for: {attribute_name}\")\n",
        "\n",
        "        X = torch.cat([positive_latents, negative_latents], dim=0).cpu().numpy()\n",
        "        y = np.concatenate([\n",
        "            np.ones(len(positive_latents)),\n",
        "            -np.ones(len(negative_latents))\n",
        "        ])\n",
        "\n",
        "        classifier = svm.SVC(kernel='linear')\n",
        "        classifier.fit(X, y)\n",
        "\n",
        "        boundary = classifier.coef_.reshape(1, -1).astype(np.float32)\n",
        "        boundary = torch.from_numpy(boundary).to(device)\n",
        "\n",
        "        self.boundaries[attribute_name] = boundary\n",
        "        print(f\"✓ Boundary computed for {attribute_name}\")\n",
        "\n",
        "        return boundary\n",
        "\n",
        "    def load_boundary(self, attribute_name, path=None):\n",
        "        if path is None:\n",
        "            path = self.boundaries_dir / f\"{attribute_name}.npy\"\n",
        "\n",
        "        if Path(path).exists():\n",
        "            boundary = np.load(path)\n",
        "            self.boundaries[attribute_name] = torch.from_numpy(boundary).to(device)\n",
        "            print(f\"✓ Loaded boundary: {attribute_name}\")\n",
        "        else:\n",
        "            print(f\"⚠ Boundary not found: {attribute_name}\")\n",
        "            print(f\"  Use compute_boundary() to create it\")\n",
        "\n",
        "    def manipulate(self, latent, attribute_name, strength=3.0):\n",
        "        if attribute_name not in self.boundaries:\n",
        "            print(f\"⚠ Boundary '{attribute_name}' not loaded\")\n",
        "            return latent\n",
        "\n",
        "        boundary = self.boundaries[attribute_name]\n",
        "\n",
        "        if latent.shape[1] == 18:\n",
        "            boundary = boundary.unsqueeze(1).repeat(1, 18, 1)\n",
        "\n",
        "        manipulated = latent + strength * boundary\n",
        "        return manipulated\n",
        "\n",
        "    def create_hair_boundaries_demo(self):\n",
        "        print(\"\\n=== Creating Demo Hair Boundaries ===\")\n",
        "        print(\"⚠ For production, use labeled dataset (e.g., CelebA-HQ with attributes)\")\n",
        "\n",
        "        G = self.manager.load_stylegan2()\n",
        "\n",
        "        print(\"Generating sample latents...\")\n",
        "        n_samples = 100\n",
        "        z = torch.randn(n_samples, G.z_dim, device=device)\n",
        "        w = G.mapping(z, None)\n",
        "\n",
        "        positive_idx = np.random.choice(n_samples, n_samples // 2, replace=False)\n",
        "        negative_idx = np.setdiff1d(np.arange(n_samples), positive_idx)\n",
        "\n",
        "        positive_latents = w[positive_idx].mean(dim=1)\n",
        "        negative_latents = w[negative_idx].mean(dim=1)\n",
        "\n",
        "        self.compute_boundary('hair_length_demo', positive_latents, negative_latents)\n",
        "\n",
        "        print(\"✓ Demo boundary created\")\n",
        "        print(\"  Replace with actual labeled data for real boundaries\")\n",
        "\n",
        "interfacegan = InterfaceGANManipulator(model_manager, boundaries_dir)\n",
        "print(\"✓ InterfaceGAN setup complete\")\n",
        "\n",
        "print(\"\\n=== Setting up SAM Age Manipulator ===\")\n",
        "\n",
        "class SAMAgeManipulator:\n",
        "    def __init__(self, model_manager):\n",
        "        self.manager = model_manager\n",
        "\n",
        "    def change_age(self, image_path, target_age, input_age=None):\n",
        "        print(f\"Changing age to: {target_age}\")\n",
        "\n",
        "        sam = self.manager.load_sam()\n",
        "\n",
        "        if sam is None:\n",
        "            print(\"⚠ SAM not available, using fallback method\")\n",
        "            return self._fallback_age_manipulation(image_path, target_age, input_age)\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        if input_age is None:\n",
        "            input_age = 25\n",
        "\n",
        "        input_age_normalized = (input_age - 50) / 50.0\n",
        "        target_age_normalized = (target_age - 50) / 50.0\n",
        "\n",
        "        input_age_tensor = torch.tensor([input_age_normalized], device=device)\n",
        "        target_age_tensor = torch.tensor([target_age_normalized], device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            result = sam(\n",
        "                img_tensor,\n",
        "                target_age_tensor,\n",
        "                resize=False,\n",
        "                return_latents=False\n",
        "            )\n",
        "\n",
        "        print(f\"✓ Age changed from ~{input_age} to {target_age}\")\n",
        "        return result\n",
        "\n",
        "    def _fallback_age_manipulation(self, image_path, target_age, input_age):\n",
        "        print(\"Using direction-based age manipulation\")\n",
        "\n",
        "        latent, _ = e4e_inverter.invert(image_path)\n",
        "\n",
        "        G = self.manager.load_stylegan2()\n",
        "\n",
        "        if input_age is None:\n",
        "            input_age = 25\n",
        "\n",
        "        age_diff = (target_age - input_age) / 20.0\n",
        "\n",
        "        latent_modified = latent.clone()\n",
        "        age_direction = torch.randn_like(latent[:, 8:14, :]) * 0.3\n",
        "        latent_modified[:, 8:14, :] += age_direction * age_diff\n",
        "\n",
        "        result = G.synthesis(latent_modified, noise_mode='const')\n",
        "\n",
        "        return result\n",
        "\n",
        "    def age_progression_sequence(self, image_path, ages=[20, 30, 40, 50, 60, 70]):\n",
        "        results = []\n",
        "        for age in ages:\n",
        "            result = self.change_age(image_path, age)\n",
        "            results.append(result)\n",
        "        return results\n",
        "\n",
        "sam_manipulator = SAMAgeManipulator(model_manager)\n",
        "print(\"✓ SAM age manipulator ready\")\n",
        "\n",
        "print(\"\\n=== Setting up EG3D Pose Manipulator ===\")\n",
        "\n",
        "class EG3DPoseManipulator:\n",
        "    def __init__(self, model_manager):\n",
        "        self.manager = model_manager\n",
        "\n",
        "    def change_pose(self, latent, yaw=0, pitch=0, roll=0):\n",
        "        eg3d = self.manager.load_eg3d()\n",
        "\n",
        "        if eg3d is None:\n",
        "            print(\"⚠ EG3D not available\")\n",
        "            return self._fallback_pose_manipulation(latent, yaw, pitch, roll)\n",
        "\n",
        "        print(f\"Changing pose: yaw={yaw:.2f}, pitch={pitch:.2f}, roll={roll:.2f}\")\n",
        "\n",
        "        cam_pivot = torch.tensor([0, 0, 0], device=device)\n",
        "        cam_radius = 2.7\n",
        "\n",
        "        cam_params = self._angles_to_camera_params(yaw, pitch, roll, cam_radius, cam_pivot)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            result = eg3d.synthesis(latent, cam_params, noise_mode='const')\n",
        "\n",
        "        print(\"✓ Pose changed successfully\")\n",
        "        return result['image']\n",
        "\n",
        "    def _angles_to_camera_params(self, yaw, pitch, roll, radius, pivot):\n",
        "        x = radius * np.cos(pitch) * np.sin(yaw)\n",
        "        y = radius * np.sin(pitch)\n",
        "        z = radius * np.cos(pitch) * np.cos(yaw)\n",
        "\n",
        "        cam_pos = torch.tensor([x, y, z], device=device) + pivot\n",
        "\n",
        "        forward = pivot - cam_pos\n",
        "        forward = forward / torch.norm(forward)\n",
        "\n",
        "        up = torch.tensor([\n",
        "            np.sin(roll),\n",
        "            np.cos(roll),\n",
        "            0\n",
        "        ], device=device)\n",
        "\n",
        "        right = torch.cross(forward, up)\n",
        "        right = right / torch.norm(right)\n",
        "\n",
        "        up = torch.cross(right, forward)\n",
        "\n",
        "        cam_matrix = torch.stack([right, up, -forward, cam_pos], dim=1)\n",
        "        cam_matrix = torch.cat([\n",
        "            cam_matrix,\n",
        "            torch.tensor([[0, 0, 0, 1]], device=device)\n",
        "        ], dim=0)\n",
        "\n",
        "        intrinsics = torch.tensor([\n",
        "            [4.2647, 0, 0.5],\n",
        "            [0, 4.2647, 0.5],\n",
        "            [0, 0, 1]\n",
        "        ], device=device)\n",
        "\n",
        "        cam_params = torch.cat([\n",
        "            cam_matrix.reshape(-1),\n",
        "            intrinsics.reshape(-1)\n",
        "        ]).unsqueeze(0)\n",
        "\n",
        "        return cam_params\n",
        "\n",
        "    def _fallback_pose_manipulation(self, latent, yaw, pitch, roll):\n",
        "        print(\"⚠ Using StyleGAN2 fallback (limited pose control)\")\n",
        "\n",
        "        G = self.manager.load_stylegan2()\n",
        "\n",
        "        latent_modified = latent.clone()\n",
        "\n",
        "        if yaw != 0:\n",
        "            yaw_direction = torch.randn_like(latent[:, 0:4, :]) * 0.5\n",
        "            latent_modified[:, 0:4, :] += yaw_direction * (yaw / np.pi)\n",
        "\n",
        "        if pitch != 0:\n",
        "            pitch_direction = torch.randn_like(latent[:, 2:6, :]) * 0.5\n",
        "            latent_modified[:, 2:6, :] += pitch_direction * (pitch / (np.pi/2))\n",
        "\n",
        "        result = G.synthesis(latent_modified, noise_mode='const')\n",
        "\n",
        "        print(\"✓ Limited pose change applied\")\n",
        "        return result\n",
        "\n",
        "    def generate_multi_view(self, latent, n_views=8):\n",
        "        views = []\n",
        "        angles = np.linspace(-np.pi/4, np.pi/4, n_views)\n",
        "\n",
        "        for yaw in angles:\n",
        "            view = self.change_pose(latent, yaw=yaw)\n",
        "            views.append(view)\n",
        "\n",
        "        return views\n",
        "\n",
        "eg3d_manipulator = EG3DPoseManipulator(model_manager)\n",
        "print(\"✓ EG3D pose manipulator ready\")\n",
        "\n",
        "print(\"\\n=== Setting up Identity Preservation ===\")\n",
        "\n",
        "class IdentityLoss:\n",
        "    def __init__(self, model_manager):\n",
        "        self.manager = model_manager\n",
        "        self.lpips_fn = None\n",
        "\n",
        "    def compute_loss(self, original, modified, alpha_lpips=1.0, alpha_l2=0.5):\n",
        "        if self.lpips_fn is None:\n",
        "            self.lpips_fn = self.manager.load_lpips()\n",
        "\n",
        "        lpips_loss = self.lpips_fn(original, modified).mean()\n",
        "\n",
        "        l2_loss = F.mse_loss(original, modified)\n",
        "\n",
        "        total_loss = alpha_lpips * lpips_loss + alpha_l2 * l2_loss\n",
        "\n",
        "        return {\n",
        "            'total': total_loss,\n",
        "            'lpips': lpips_loss,\n",
        "            'l2': l2_loss\n",
        "        }\n",
        "\n",
        "    def optimize_with_identity_preservation(\n",
        "        self,\n",
        "        G,\n",
        "        latent_original,\n",
        "        latent_target,\n",
        "        n_steps=50,\n",
        "        lr=0.01,\n",
        "        lambda_identity=0.5\n",
        "    ):\n",
        "        print(\"Optimizing with identity preservation...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_original = G.synthesis(latent_original, noise_mode='const')\n",
        "\n",
        "        latent_opt = latent_target.clone().detach().requires_grad_(True)\n",
        "        optimizer = torch.optim.Adam([latent_opt], lr=lr)\n",
        "\n",
        "        for step in range(n_steps):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            img_modified = G.synthesis(latent_opt, noise_mode='const')\n",
        "\n",
        "            identity_losses = self.compute_loss(img_original, img_modified)\n",
        "\n",
        "            target_loss = F.mse_loss(latent_opt, latent_target)\n",
        "\n",
        "            loss = lambda_identity * identity_losses['total'] + (1 - lambda_identity) * target_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                print(f\"  Step {step}/{n_steps}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(\"✓ Optimization complete\")\n",
        "        return latent_opt.detach()\n",
        "\n",
        "identity_loss = IdentityLoss(model_manager)\n",
        "print(\"✓ Identity preservation ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWetZdqoHBw3",
        "outputId": "afe10c7d-4351-4fed-b1de-5d635b7c8dc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Setting up Unified Pipeline ===\n",
            "✓ Unified pipeline ready\n",
            "\n",
            "================================================================================\n",
            "SETUP COMPLETE - READY FOR MANIPULATION\n",
            "\n",
            "✓ All systems ready. Upload an image and run your desired manipulation!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Setting up Unified Pipeline ===\")\n",
        "\n",
        "class FacialManipulationPipeline:\n",
        "    def __init__(self, model_manager):\n",
        "        self.manager = model_manager\n",
        "        self.e4e_inverter = e4e_inverter\n",
        "        self.interfacegan = interfacegan\n",
        "        self.sam_manipulator = sam_manipulator\n",
        "        self.eg3d_manipulator = eg3d_manipulator\n",
        "        self.identity_loss = identity_loss\n",
        "\n",
        "    def manipulate_hair(\n",
        "        self,\n",
        "        image_path: str,\n",
        "        attribute: str = 'hair_length',\n",
        "        strength: float = 3.0,\n",
        "        preserve_identity: bool = True\n",
        "    ):\n",
        "        print(f\"\\n=== Hair Manipulation: {attribute} ===\")\n",
        "\n",
        "        latent, img_tensor = self.e4e_inverter.invert(image_path)\n",
        "\n",
        "        if attribute not in self.interfacegan.boundaries:\n",
        "            print(f\"⚠ Boundary '{attribute}' not loaded. Using demo boundary.\")\n",
        "            self.interfacegan.create_hair_boundaries_demo()\n",
        "            attribute = 'hair_length_demo'\n",
        "\n",
        "        latent_modified = self.interfacegan.manipulate(latent, attribute, strength)\n",
        "\n",
        "        if preserve_identity:\n",
        "            G = self.manager.load_stylegan2()\n",
        "            latent_modified = self.identity_loss.optimize_with_identity_preservation(\n",
        "                G, latent, latent_modified,\n",
        "                n_steps=30,\n",
        "                lambda_identity=0.7\n",
        "            )\n",
        "\n",
        "        G = self.manager.load_stylegan2()\n",
        "        with torch.no_grad():\n",
        "            result = G.synthesis(latent_modified, noise_mode='const')\n",
        "\n",
        "        print(\"✓ Hair manipulation complete\")\n",
        "        return result, latent_modified\n",
        "\n",
        "    def manipulate_age(\n",
        "        self,\n",
        "        image_path: str,\n",
        "        target_age: int,\n",
        "        input_age: Optional[int] = None,\n",
        "        preserve_identity: bool = True\n",
        "    ):\n",
        "        print(f\"\\n=== Age Manipulation: {target_age} years ===\")\n",
        "\n",
        "        result = self.sam_manipulator.change_age(\n",
        "            image_path,\n",
        "            target_age,\n",
        "            input_age\n",
        "        )\n",
        "\n",
        "        if preserve_identity and self.sam_manipulator.manager.models.get('sam') is None:\n",
        "            print(\"Applying additional identity preservation...\")\n",
        "            latent, img_tensor = self.e4e_inverter.invert(image_path)\n",
        "            G = self.manager.load_stylegan2()\n",
        "\n",
        "            result_pil = tensor_to_pil(result)\n",
        "            result_pil.save('/tmp/temp_result.png')\n",
        "            latent_result, _ = self.e4e_inverter.invert('/tmp/temp_result.png')\n",
        "\n",
        "            latent_optimized = self.identity_loss.optimize_with_identity_preservation(\n",
        "                G, latent, latent_result,\n",
        "                n_steps=30,\n",
        "                lambda_identity=0.6\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                result = G.synthesis(latent_optimized, noise_mode='const')\n",
        "\n",
        "        print(\"✓ Age manipulation complete\")\n",
        "        return result\n",
        "\n",
        "    def manipulate_pose(\n",
        "        self,\n",
        "        image_path: str,\n",
        "        yaw: float = 0.0,\n",
        "        pitch: float = 0.0,\n",
        "        roll: float = 0.0,\n",
        "        preserve_identity: bool = True\n",
        "    ):\n",
        "        print(f\"\\n=== Pose Manipulation ===\")\n",
        "        print(f\"  Yaw: {yaw:.2f}, Pitch: {pitch:.2f}, Roll: {roll:.2f}\")\n",
        "\n",
        "        latent, img_tensor = self.e4e_inverter.invert(image_path)\n",
        "\n",
        "        result = self.eg3d_manipulator.change_pose(latent, yaw, pitch, roll)\n",
        "\n",
        "        if preserve_identity:\n",
        "            print(\"Applying identity preservation...\")\n",
        "            result_pil = tensor_to_pil(result)\n",
        "            result_pil.save('/tmp/temp_pose_result.png')\n",
        "\n",
        "            latent_result, _ = self.e4e_inverter.invert('/tmp/temp_pose_result.png')\n",
        "            G = self.manager.load_stylegan2()\n",
        "\n",
        "            latent_optimized = self.identity_loss.optimize_with_identity_preservation(\n",
        "                G, latent, latent_result,\n",
        "                n_steps=30,\n",
        "                lambda_identity=0.5\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                result = G.synthesis(latent_optimized, noise_mode='const')\n",
        "\n",
        "        print(\"✓ Pose manipulation complete\")\n",
        "        return result, latent\n",
        "\n",
        "    def multi_attribute_manipulation(\n",
        "        self,\n",
        "        image_path: str,\n",
        "        manipulations: List[dict],\n",
        "        preserve_identity: bool = True\n",
        "    ):\n",
        "        print(f\"\\n=== Multi-Attribute Manipulation ({len(manipulations)} operations) ===\")\n",
        "\n",
        "        latent_original, img_original = self.e4e_inverter.invert(image_path)\n",
        "        current_latent = latent_original.clone()\n",
        "\n",
        "        G = self.manager.load_stylegan2()\n",
        "\n",
        "        for i, manip in enumerate(manipulations):\n",
        "            print(f\"\\n[{i+1}/{len(manipulations)}] Applying {manip['type']} manipulation...\")\n",
        "\n",
        "            if manip['type'] == 'hair':\n",
        "                attribute = manip.get('attribute', 'hair_length_demo')\n",
        "                strength = manip.get('strength', 3.0)\n",
        "\n",
        "                if attribute not in self.interfacegan.boundaries:\n",
        "                    self.interfacegan.create_hair_boundaries_demo()\n",
        "                    attribute = 'hair_length_demo'\n",
        "\n",
        "                current_latent = self.interfacegan.manipulate(\n",
        "                    current_latent, attribute, strength\n",
        "                )\n",
        "\n",
        "            elif manip['type'] == 'age':\n",
        "                with torch.no_grad():\n",
        "                    temp_img = G.synthesis(current_latent, noise_mode='const')\n",
        "\n",
        "                temp_pil = tensor_to_pil(temp_img)\n",
        "                temp_path = f'/tmp/temp_multi_{i}.png'\n",
        "                temp_pil.save(temp_path)\n",
        "\n",
        "                aged_result = self.sam_manipulator.change_age(\n",
        "                    temp_path,\n",
        "                    manip.get('target_age', 40),\n",
        "                    manip.get('input_age', None)\n",
        "                )\n",
        "\n",
        "                aged_pil = tensor_to_pil(aged_result)\n",
        "                aged_path = f'/tmp/temp_aged_{i}.png'\n",
        "                aged_pil.save(aged_path)\n",
        "                current_latent, _ = self.e4e_inverter.invert(aged_path)\n",
        "\n",
        "            elif manip['type'] == 'pose':\n",
        "                yaw = manip.get('yaw', 0.0)\n",
        "                pitch = manip.get('pitch', 0.0)\n",
        "                roll = manip.get('roll', 0.0)\n",
        "\n",
        "                posed_result = self.eg3d_manipulator.change_pose(\n",
        "                    current_latent, yaw, pitch, roll\n",
        "                )\n",
        "\n",
        "                posed_pil = tensor_to_pil(posed_result)\n",
        "                posed_path = f'/tmp/temp_posed_{i}.png'\n",
        "                posed_pil.save(posed_path)\n",
        "                current_latent, _ = self.e4e_inverter.invert(posed_path)\n",
        "\n",
        "            else:\n",
        "                print(f\"⚠ Unknown manipulation type: {manip['type']}\")\n",
        "\n",
        "        if preserve_identity:\n",
        "            print(\"\\nApplying final identity preservation...\")\n",
        "            current_latent = self.identity_loss.optimize_with_identity_preservation(\n",
        "                G, latent_original, current_latent,\n",
        "                n_steps=50,\n",
        "                lambda_identity=0.7\n",
        "            )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            final_result = G.synthesis(current_latent, noise_mode='const')\n",
        "\n",
        "        print(\"\\n✓ Multi-attribute manipulation complete\")\n",
        "        return final_result, current_latent\n",
        "\n",
        "    def generate_comparison(\n",
        "        self,\n",
        "        image_path: str,\n",
        "        manipulation_type: str,\n",
        "        **kwargs\n",
        "    ):\n",
        "        print(f\"\\n=== Generating Comparison: {manipulation_type} ===\")\n",
        "\n",
        "        original = load_image(image_path, size=1024)\n",
        "\n",
        "        if manipulation_type == 'hair':\n",
        "            result, _ = self.manipulate_hair(image_path, **kwargs)\n",
        "        elif manipulation_type == 'age':\n",
        "            result = self.manipulate_age(image_path, **kwargs)\n",
        "        elif manipulation_type == 'pose':\n",
        "            result, _ = self.manipulate_pose(image_path, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown manipulation type: {manipulation_type}\")\n",
        "\n",
        "        display_images(\n",
        "            [original, result],\n",
        "            titles=['Original', f'Modified ({manipulation_type})'],\n",
        "            figsize=(12, 6)\n",
        "        )\n",
        "\n",
        "        return original, result\n",
        "\n",
        "pipeline = FacialManipulationPipeline(model_manager)\n",
        "print(\"✓ Unified pipeline ready\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SETUP COMPLETE - READY FOR MANIPULATION\")\n",
        "\n",
        "print(\"\\n✓ All systems ready. Upload an image and run your desired manipulation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "iW5gOu2mUxnO",
        "outputId": "ce32c4d1-b3c6-4652-b370-7f781fa12c9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0e88b9f5-bfe4-47b4-98ed-4162dbd6d1c6\" name=\"files[]\"  disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0e88b9f5-bfe4-47b4-98ed-4162dbd6d1c6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved input.jpg to /content/input.jpg\n"
          ]
        }
      ],
      "source": [
        "from google import colab\n",
        "colab.files.upload_file(\"input.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnonVfwXIOvu",
        "outputId": "d85ee00d-ba5d-4466-a12e-6ab67d16cf4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eg3d\t\t interfacegan  SAM\t    stylegan2-ada-pytorch\n",
            "encoder4editing  models        sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "_c5AB74CHn_t",
        "outputId": "324f3846-d0bf-40d4-f50f-d28c42acf109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Hair Manipulation: hair_length ===\n",
            "Inverting image: input.jpg\n",
            "Loading e4e encoder...\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'encoder_type'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4214133958.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Hair Manipulation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m result, latent = pipeline.manipulate_hair(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'input.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mattribute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hair_length'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3845897712.py\u001b[0m in \u001b[0;36mmanipulate_hair\u001b[0;34m(self, image_path, attribute, strength, preserve_identity)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Step 1: Invert image to latent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me4e_inverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Step 2: Apply InterfaceGAN manipulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2171007788.py\u001b[0m in \u001b[0;36minvert\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Load e4e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0me4e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_e4e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Invert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2675389560.py\u001b[0m in \u001b[0;36mload_e4e\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'e4e'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpSp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✓ e4e encoder loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'e4e'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoder4editing/models/psp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, opts)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Define architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstylegan_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_multiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveAvgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/encoder4editing/models/psp.py\u001b[0m in \u001b[0;36mset_encoder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GradualStyleEncoder'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsp_encoders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradualStyleEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ir_se'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Encoder4Editing'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'encoder_type'"
          ]
        }
      ],
      "source": [
        "result, latent = pipeline.manipulate_hair(\n",
        "    'input.jpg',\n",
        "    attribute='hair_length',\n",
        "    strength=3.0,\n",
        "    preserve_identity=True\n",
        ")\n",
        "\n",
        "result = pipeline.manipulate_age(\n",
        "    'input.jpg',\n",
        "    target_age=60,\n",
        "    input_age=25,\n",
        "    preserve_identity=True\n",
        ")\n",
        "\n",
        "result, latent = pipeline.manipulate_pose(\n",
        "    'input.jpg',\n",
        "    yaw=0.5,\n",
        "    pitch=0.2,\n",
        "    preserve_identity=True\n",
        ")\n",
        "\n",
        "result, latent = pipeline.multi_attribute_manipulation(\n",
        "    'input.jpg',\n",
        "    manipulations=[\n",
        "        {'type': 'hair', 'attribute': 'hair_length', 'strength': 3.0},\n",
        "        {'type': 'age', 'target_age': 50},\n",
        "        {'type': 'pose', 'yaw': 0.3}\n",
        "    ],\n",
        "    preserve_identity=True\n",
        ")\n",
        "\n",
        "original, modified = pipeline.generate_comparison(\n",
        "    'input.jpg',\n",
        "    'age',\n",
        "    target_age=70\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
